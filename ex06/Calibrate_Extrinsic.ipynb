{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration of a End-of-Arm Camera\n",
    "\n",
    "This exercise will describe how to calibrate a end-of-arm camera using marker-board detection.\n",
    "The excercise is divided into the following steps.\n",
    "\n",
    "1. Load recorded images.\n",
    "2. Show marker detection results.\n",
    "3. Solve for the calibration.\n",
    "4. Analysis of results.\n",
    "\n",
    "First lets have a look at the data.\n",
    "\n",
    "Note: if the interactive viewer does not work you may have to restart the notebook with `%matplotlib widget` instead of `%matplotlib notebook`\n",
    "\n",
    "Note: install jupyter in your conda environment via\n",
    "`conda install -c conda-forge notebook` and the other missing packages.\n",
    "Start it in the command line via `jupter notebook --port xxxx`. Use the vs-code pop up to open jupyter in your local server.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='calibration_ls.png')  # image adapted from torsteinmyhre.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise want to do eye-in-hand extrinsic calibration. In this setup, a camera (eye) is mounted at end of a robot arm. The end of the robot arm is called Tool Center Point (TCP) because this is where tools are mounted. We would like to find the transformation `T_cam_tcp`. We do this calibration by moving the robot and taking several pictures of the marker board.\n",
    "\n",
    "In the setup, the marker board, and the base of the robot are static, this means that `T_cam_tcp` as well as `T_robot_marker_marker` are fixed. For each view recorded the robot arm is in a different location, which changes the position of the marker board as seen by the camera. This means that `T_tcp_robot` and `T_cam_marker` change for each view. Calibration works by finding a loop in these poses, and solving for `T_cam_tcp`. This can be done in several ways, in this exercise we present to methods, a simple least-squares optimiation and the peak-martin algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "#%matplotlib widget\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "try:\n",
    "    import open3d as o3d\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data required for this exercise (30Mb)\n",
    "! wget https://lmb.informatik.uni-freiburg.de/people/argusm/marker.tar\n",
    "! tar -xvf marker.tar\n",
    "# Update: please find the files under `/project/cv-ws2122/shared-data1/marker`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Recorded Images\n",
    "\n",
    "This cell defines a data class that loads images and data from files.\n",
    "\n",
    "Please complete the `get_projection_matrix` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "class ViewLoader:\n",
    "    def __init__(self, base_path):\n",
    "        self.base_path = base_path\n",
    "        assert os.path.isdir(base_path)\n",
    "        files = sorted(os.listdir(self.base_path))\n",
    "        files = [f for f in files if (f.startswith(\"rgb_\") and f.endswith(\".png\"))]\n",
    "        self.max_idx = int(files[-1].replace(\"rgb_\", \"\").replace(\".png\", \"\"))\n",
    "        print(f\"Loaded {self.max_idx+1} images.\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.max_idx + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.get_rgbdp(idx)\n",
    "\n",
    "    def get_info(self):\n",
    "        info_path = os.path.join(self.base_path, 'info.json')\n",
    "        with open(info_path, \"rb\") as f_obj:\n",
    "            info  = json.load(f_obj)\n",
    "        return info\n",
    "    \n",
    "    def get_intrinsics(self):\n",
    "        info = self.get_info()\n",
    "        calib = info[\"camera\"][\"calibration\"]\n",
    "        return calib\n",
    "    \n",
    "    def get_K(self):\n",
    "        calib = self.get_intrinsics()\n",
    "        cam_intrinsic = np.eye(3)\n",
    "        cam_intrinsic[0, 0] = calib[\"fx\"]\n",
    "        cam_intrinsic[1, 1] = calib[\"fy\"]\n",
    "        cam_intrinsic[0, 2] = calib[\"ppx\"]\n",
    "        cam_intrinsic[1, 2] = calib[\"ppy\"]\n",
    "        return cam_intrinsic\n",
    "    \n",
    "    def get_robot_pose(self, idx, return_dict=False):\n",
    "        pose_file = os.path.join(self.base_path, \"pose_{0:04d}.json\".format(idx) )\n",
    "        with open(pose_file,\"rb\") as f_obj:\n",
    "            pose = json.load(f_obj)\n",
    "        pose_m = np.eye(4)\n",
    "        pose_m[:3, :3] = R.from_euler(\"xyz\", [pose[x] for x in ['rot_x', 'rot_y', 'rot_z']]).as_matrix()\n",
    "        pose_m[:3, 3] = [pose[x] for x in ['x', 'y', 'z']]\n",
    "        if return_dict:\n",
    "            return pose_m, pose\n",
    "        else:\n",
    "            return pose_m\n",
    "    \n",
    "    # read RGB image\n",
    "    def get_rgb_file(self, idx):\n",
    "        rgb_file = os.path.join(self.base_path, \"rgb_{0:04d}.png\".format(idx) )\n",
    "        return rgb_file\n",
    "    \n",
    "    # read depth image\n",
    "    def get_depth_file(self, idx):\n",
    "        depth_file = os.path.join(self.base_path, \"depth_{0:04d}.png\".format(idx) )\n",
    "        return depth_file    \n",
    "    \n",
    "    # get RGB image, scaled-depth image and robot-pose.\n",
    "    def get_rgbdp(self, idx):\n",
    "        rgb_file = self.get_rgb_file(idx)\n",
    "        depth_file = self.get_depth_file(idx)\n",
    "        \n",
    "        pose_m, pose_d = self.get_robot_pose(idx, True)\n",
    "        # depth\n",
    "        depth_scaling = pose_d[\"depth_scaling\"]\n",
    "        rgb  = np.asarray(Image.open(rgb_file))\n",
    "        depth = np.asarray(Image.open(depth_file), dtype=np.float32) * depth_scaling\n",
    "        return rgb, depth, pose_m\n",
    "    \n",
    "    # get camera pose\n",
    "    def get_cam_pose(self, idx, marker_dir=\"pose_marker_one\"):\n",
    "        marker_dir = os.path.join(self.base_path, marker_dir)\n",
    "        fn  = \"{0:08d}.json\".format(idx)\n",
    "        pose_fn = os.path.join(marker_dir, fn)\n",
    "        with open(pose_fn, \"r\") as fo:\n",
    "            T = np.array(json.load(fo))\n",
    "        return T\n",
    "    \n",
    "    def get_projection_matrix(self):\n",
    "        # returns a 4x3 projection matrix using the intrinsics\n",
    "        # TODO\n",
    "        raise NotImplemented\n",
    "        # end todo\n",
    "        assert cam_mat.shape == (3, 4)\n",
    "        return cam_mat\n",
    "    \n",
    "\n",
    "    def project(self, X):\n",
    "        \"\"\"\n",
    "        Project an (homogenous) cartesian coordinate into the camera frame.\n",
    "        \"\"\"\n",
    "        if X.shape[0] == 3:\n",
    "            if len(X.shape) == 1:\n",
    "                X = np.append(X, 1)\n",
    "            else:\n",
    "                X = np.concatenate([X, np.ones((1, X.shape[1]))], axis=0)\n",
    "\n",
    "        x = self.get_projection_matrix() @ X\n",
    "        result = np.round(x[0:2] / x[2]).astype(int)\n",
    "        width, height = self.get_intrinsics()['width'], self.get_intrinsics()['height']\n",
    "        if not (0 <= result[0] < width and 0 <= result[1] < height):\n",
    "            log.warning(\"Projected point outside of image bounds\")\n",
    "        return result[0], result[1]\n",
    "\n",
    "vl = ViewLoader(base_path=\"marker\")\n",
    "print(\"camera calibration:\")\n",
    "camera_calibration = vl.get_K()\n",
    "K = np.array(camera_calibration)\n",
    "print(K.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets, Layout, interact\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "image, depth, pose = vl.get_rgbdp(0)\n",
    "line = ax.imshow(np.asarray(image))\n",
    "ax.set_axis_off()\n",
    "\n",
    "def update(w):\n",
    "    image, depth, pose = vl.get_rgbdp(w)\n",
    "    line.set_data(np.asarray(image))\n",
    "    fig.canvas.draw_idle()\n",
    "    \n",
    "slider_w = widgets.IntSlider(min=0, max=len(vl)-1, step=1, value=0,\n",
    "                             layout=Layout(width='70%'))\n",
    "interact(update, w=slider_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Show Marker Detection Results.\n",
    "\n",
    "To simplify things marker detection has already been run. Next we want to verify its results.\n",
    "Do this by completing the `get_projection_matrix` function in the ViewLoader.\n",
    "Then draw a coordinate frame into each image for which we have detection results.\n",
    "The coordinate frame should have axis lengths of 10cm, with x=red, y=green, and z=blue.\n",
    "This can be done using `PIL.ImageDraw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "\n",
    "def show_marker_pose(image, T_cam_object):\n",
    "    \"\"\"\n",
    "    draw the coordinate frame into each image for which we have detection results\n",
    "    Arguments:\n",
    "        image: image as numpy.ndarray\n",
    "        T_cam_object: transform from object into cam x_cam = T_cam_object @ x\n",
    "    Returns:\n",
    "        im: image (should be PIL.Image.Image)\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    #  using PIL.ImageDraw\n",
    "    # 1. Define <x, y, z, center> coordinates\n",
    "    # 2. Transform <x, y, z, center> to into camera frame\n",
    "    # 3. Project homogenous coordinates <cam_x, cam_y, cam_z, center> to camera image\n",
    "    raise NotImplemented\n",
    "    # end TODO\n",
    "    return im\n",
    "\n",
    "image, depth, robot_pose = vl.get_rgbdp(1)\n",
    "T_cam_object = vl.get_cam_pose(1)\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "image_m = show_marker_pose(image, T_cam_object)\n",
    "line = ax.imshow(np.asarray(image_m))\n",
    "ax.set_axis_off()\n",
    "\n",
    "def update(w):\n",
    "    image, depth, pose = vl.get_rgbdp(w)\n",
    "    try:\n",
    "        T_cam_object = vl.get_cam_pose(w)\n",
    "    except FileNotFoundError:\n",
    "        print(\"No pose estimation.\")\n",
    "        line.set_data(np.asarray(image))\n",
    "        return\n",
    "    image_m = show_marker_pose(image, T_cam_object)\n",
    "    line.set_data(np.asarray(image_m))\n",
    "    fig.canvas.draw_idle()\n",
    "    \n",
    "slider_w = widgets.IntSlider(min=0, max=len(vl)-1, step=1, value=0,\n",
    "                             layout=Layout(width='70%'))\n",
    "interact(update, w=slider_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tcp_marker_lists(m_dir=\"pose_marker_one\"):\n",
    "    T_robot_tcp_list = []\n",
    "    T_cam_marker_list = []\n",
    "    for i in range(len(vl)):\n",
    "        try:\n",
    "            robot_pose = vl.get_robot_pose(i)\n",
    "            cam_pose = vl.get_cam_pose(i, marker_dir=m_dir)\n",
    "        except (FileNotFoundError, ValueError):\n",
    "            continue\n",
    "        T_robot_tcp_list.append(robot_pose)\n",
    "        T_cam_marker_list.append(cam_pose)\n",
    "\n",
    "    return np.array(T_robot_tcp_list), np.array(T_cam_marker_list)\n",
    "\n",
    "#plot_o3d = True\n",
    "plot_o3d = False\n",
    "if plot_o3d:\n",
    "    T_robot_tcp_list, T_cam_marker_list = get_tcp_marker_lists()\n",
    "    mesh_frames = []\n",
    "    for T_robot_tcp, T_cam_marker in zip(T_robot_tcp_list, T_cam_marker_list):\n",
    "        mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.1)\n",
    "        mesh_frame.transform(np.linalg.inv(T_robot_tcp))\n",
    "        mesh_frames.append(mesh_frame)\n",
    "        \n",
    "        mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.20)\n",
    "        mesh_frame.transform(T_cam_marker)\n",
    "        mesh_frames.append(mesh_frame)\n",
    "\n",
    "    o3d.visualization.draw_geometries(mesh_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Solve for the Calibration\n",
    "\n",
    "We will try to find the calibration using two different methods, via least squares optimization and using the park martin algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_to_matrix(x):\n",
    "    mat = np.eye(4)\n",
    "    mat[:3, 3] = x[0:3]\n",
    "    mat[:3, :3] = R.from_euler('xyz', x[3:6]).as_matrix()\n",
    "    return mat\n",
    "\n",
    "def pprint(arr):\n",
    "    return np.array2string(arr.round(5), separator=', ')\n",
    "\n",
    "def matrix_to_pos_orn(mat):\n",
    "    \"\"\"\n",
    "    :param mat: 4x4 homogeneous transformation\n",
    "    :return: tuple(position: np.array of shape (3,), orientation: np.array of shape (4,) -> quaternion xyzw)\n",
    "    \"\"\"\n",
    "    orn = R.from_matrix(mat[:3, :3]).as_quat()\n",
    "    pos = mat[:3, 3]\n",
    "    return pos, orn\n",
    "\n",
    "def calculate_error(T_tcp_cam, T_robot_tcp_list, T_cam_marker_list, inliers=None):\n",
    "    \"\"\"\n",
    "    returns scalar error for each entry\n",
    "    \"\"\"\n",
    "    T_robot_marker_list = []\n",
    "    for T_robot_tcp, T_cam_marker in zip(T_robot_tcp_list, T_cam_marker_list):\n",
    "        T_robot_marker = T_robot_tcp @ T_tcp_cam @ T_cam_marker\n",
    "        T_robot_marker_list.append(T_robot_marker)\n",
    "    \n",
    "    poses = np.array(T_robot_marker_list)[:, :3, 3]\n",
    "    if inliers is None:\n",
    "        mean_pose = np.mean(poses, axis=0)\n",
    "    else:\n",
    "        mean_pose = np.mean(poses[inliers], axis=0)\n",
    "    err = np.sum((poses - mean_pose)**2, axis=1)\n",
    "    return err\n",
    "\n",
    "def pose_error(T_tcp_cam, T_robot_tcp_list, T_cam_marker_list):\n",
    "    \"\"\"\n",
    "    returns position tuple for each entry\n",
    "    \n",
    "    Arguments:\n",
    "        T_tcp_cam: cam to tcp transform\n",
    "        T_robot_tcp_list: robot base <- tcp transforms, shape (N, 4, 4 )\n",
    "        T_cam_marker_list: cam <- marker transforms, shape (N, 4, 4)\n",
    "    Returns:\n",
    "        error: cartesian error (N, 3)\n",
    "    \"\"\"\n",
    "    T_robot_marker_list = []\n",
    "    for T_robot_tcp, T_cam_marker in zip(T_robot_tcp_list, T_cam_marker_list):\n",
    "        T_robot_marker = T_robot_tcp @ T_tcp_cam @ T_cam_marker\n",
    "        T_robot_marker_list.append(T_robot_marker)\n",
    "    \n",
    "    poses = np.array(T_robot_marker_list)[:, :3, 3]\n",
    "    err = poses - np.mean(poses, axis=0)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least-Squares Optimization\n",
    "\n",
    "We know that `T_robot_marker` and `T_cam_tcp` are fixed, so these can be used as variables that have to be optimized.\n",
    "From these values compute the predicted `T_cam_marker_pred` and compare it to `T_cam_marker_obs`.\n",
    "We can do the comparison based only on position values, so `T_robot_marker` only uses position components.\n",
    "Optimize the function using `scipy.optimize.least_squares`.\n",
    "\n",
    "`T_cam_marker = T_cam_tcp @ T_tcp_robot @ T_robot_marker`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import least_squares, minimize\n",
    "\n",
    "def compute_residuals_gripper_cam(x, T_robot_tcp_list, T_cam_marker_list):\n",
    "    # calculate predicted values using T_cam_tcp and T_robot_marker\n",
    "    # use these predicted values along with the observes values to calculate and returns the residuals\n",
    "    T_robot_marker = np.array([*x[6:], 1])\n",
    "    T_cam_tcp = vec_to_matrix(x)\n",
    "    \n",
    "    # TODO\n",
    "    raise NotImplemented\n",
    "    assert len(residuals) == 144  # 144= 48 samples * 3 x,y,z\n",
    "    return residuals\n",
    "\n",
    "def calibrate_gripper_cam_ls(T_robot_tcp_list, T_cam_marker_list):\n",
    "    # use scipy least squares to optimize the above function and return the calibration\n",
    "    # todo\n",
    "    x0 = np.array([0, 0, 0, 0, 0, 0, 0, 0, -0.1])\n",
    "    result = least_squares(fun=compute_residuals_gripper_cam, x0=x0, method='lm',\n",
    "                           args=(T_robot_tcp_list, T_cam_marker_list))\n",
    "    T_calib = np.linalg.inv(vec_to_matrix(result.x))\n",
    "    \n",
    "    # end todo\n",
    "    assert T_calib.shape == (4, 4)\n",
    "    return T_calib\n",
    "\n",
    "T_robot_tcp_list, T_cam_marker_list = get_tcp_marker_lists()\n",
    "T_calib = calibrate_gripper_cam_ls(T_robot_tcp_list, T_cam_marker_list)\n",
    "err_ls = pose_error(T_calib, T_robot_tcp_list, T_cam_marker_list)\n",
    "err_ls_s = np.sum(err_ls**2, axis=1)\n",
    "print(\"median error ls\", np.median(err_ls_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to slighlty improve the result using differential evolution\n",
    "# nothing required here.\n",
    "\n",
    "import scipy\n",
    "def calibrate_gripper_cam_de(T_robot_tcp_list, T_cam_marker_list):\n",
    "    x0 = np.array([0, 0, 0, 0, 0, 0, 0, 0, -0.1])\n",
    "    result = least_squares(fun=compute_residuals_gripper_cam, x0=x0, method='lm',\n",
    "                           args=(T_robot_tcp_list, T_cam_marker_list))\n",
    "    T_calib = np.linalg.inv(vec_to_matrix(result.x))\n",
    "    \n",
    "    x0 = result.x\n",
    "    bounds = [(x-.0001, x+.0001) for x in x0]\n",
    "    def func(x, *args):\n",
    "        T_cam_tcp = vec_to_matrix(x)\n",
    "        return calculate_error(T_cam_tcp, *args).mean()\n",
    "    result2 = scipy.optimize.differential_evolution(func=func, bounds=bounds,\n",
    "                           args=(T_robot_tcp_list, T_cam_marker_list), tol=1e-11)\n",
    "    T_calib = np.linalg.inv(vec_to_matrix(result2.x))\n",
    "    #print(result.x)\n",
    "    #print(bounds)\n",
    "    return T_calib\n",
    "\n",
    "T_calib = calibrate_gripper_cam_de(T_robot_tcp_list, T_cam_marker_list)\n",
    "err_ls = pose_error(T_calib, T_robot_tcp_list, T_cam_marker_list)\n",
    "err_ls_s = np.sum(err_ls**2, axis=1)\n",
    "print(\"median error ls\", np.median(err_ls_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Park-Martin Calibration\n",
    "\n",
    "Have a look here for the description of the Park-Martin method.\n",
    "\n",
    "https://www.torsteinmyhre.name/snippets/robcam_calibration.html\n",
    "\n",
    "We are trying to solve the following equation: `AX = XB`. Follow the instructions on the website to create the lists `As`,`Bs`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def log_func(R):\n",
    "    # Rotation matrix logarithm\n",
    "    theta = np.arccos((R[0,0] + R[1,1] + R[2,2] - 1.0)/2.0)\n",
    "    return np.array([R[2,1] - R[1,2], R[0,2] - R[2,0], R[1,0] - R[0,1]]) * theta / (2*np.sin(theta))\n",
    "\n",
    "def invsqrt(mat):\n",
    "    u,s,v = np.linalg.svd(mat)\n",
    "    return u.dot(np.diag(1.0/np.sqrt(s))).dot(v)\n",
    "\n",
    "def calibrate(A, B):\n",
    "    #transform pairs A_i, B_i\n",
    "    N = len(A)\n",
    "    M = np.zeros((3, 3))\n",
    "    for i in range(N):\n",
    "        Ra, Rb = A[i][0:3, 0:3], B[i][0:3, 0:3]\n",
    "        M += np.outer(log_func(Rb), log_func(Ra))\n",
    "\n",
    "    Rx = np.dot(invsqrt(np.dot(M.T, M)), M.T)\n",
    "\n",
    "    C = np.zeros((3*N, 3))\n",
    "    d = np.zeros((3*N, 1))\n",
    "    for i in range(N):\n",
    "        Ra, ta = A[i][0:3, 0:3], A[i][0:3, 3]\n",
    "        Rb, tb = B[i][0:3, 0:3], B[i][0:3, 3]\n",
    "        C[3*i:3*i+3, :] = np.eye(3) - Ra\n",
    "        d[3*i:3*i+3, 0] = ta - np.dot(Rx, tb)\n",
    "\n",
    "    tx = np.dot(np.linalg.inv(np.dot(C.T, C)), np.dot(C.T, d))    \n",
    "    X = np.eye(4)\n",
    "    X[:3, :3] = Rx\n",
    "    X[:3, 3] = tx.flatten()\n",
    "    return X\n",
    "\n",
    "def calibrate_gripper_cam_peak_martin(T_robot_tcp_list, T_cam_marker_list):\n",
    "    ECs = []\n",
    "    for T_robot_tcp, t_cam_marker in zip(T_robot_tcp_list, T_cam_marker_list):\n",
    "        T_tcp_robot = np.linalg.inv(T_robot_tcp)\n",
    "        ECs.append((T_tcp_robot, t_cam_marker))\n",
    "\n",
    "    As = []  # relative EEs\n",
    "    Bs = []  # relative cams\n",
    "    for pair in itertools.combinations(ECs, 2):\n",
    "        (e_1, c_1), (e_2, c_2) = pair\n",
    "        A = e_2 @ np.linalg.inv(e_1)\n",
    "        B = c_2 @ np.linalg.inv(c_1)\n",
    "        As.append(A)\n",
    "        Bs.append(B)\n",
    "\n",
    "        # symmetrize\n",
    "        A = e_1 @ np.linalg.inv(e_2)\n",
    "        B = c_1 @ np.linalg.inv(c_2)\n",
    "        As.append(A)\n",
    "        Bs.append(B)\n",
    "    \n",
    "    X = calibrate(As, Bs)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Analysis of results\n",
    "\n",
    "Lets have a look at the results of both methods. \n",
    "\n",
    "1. How much does each datapoint contribute to the overall error? How much does each cartesian dimension contribute to the overall error?\n",
    "2. Does the error correlate with distance from the camera?\n",
    "3. How do the results change if we remove high error datapoints?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_robot_tcp_list, T_cam_marker_list = get_tcp_marker_lists()\n",
    "T_calib = calibrate_gripper_cam_ls(T_robot_tcp_list, T_cam_marker_list)\n",
    "err_ls = pose_error(T_calib, T_robot_tcp_list, T_cam_marker_list)\n",
    "\n",
    "T_calib = calibrate_gripper_cam_peak_martin(T_robot_tcp_list, T_cam_marker_list)\n",
    "err_pm = pose_error(T_calib, T_robot_tcp_list, T_cam_marker_list)\n",
    "\n",
    "\n",
    "#TODO make plots here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error-Depth Correlation\n",
    "\n",
    "Does the error correlate with distance from the camera?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make plots here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Remove High-Error Samples.\n",
    "\n",
    "How do the results change if we remove high error datapoints?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Make plots here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Complete\n",
    "\n",
    "The addtional material is provided as a quick introduction into Open3D. There is nothing to do there, run the code if you have time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally lets plot errors as a funciton of T_robot_tcp positions\n",
    "# This can be done using the open3d create_coordinate_frame command.\n",
    "mesh_frames = []\n",
    "err_ls = pose_error(T_calib, T_robot_tcp_list, T_cam_marker_list)\n",
    "err_ls_s = np.sum(err_ls**2, axis=1)\n",
    "err_ls_sn = err_ls_s / np.sum(err_ls_s)\n",
    "for T_robot_tcp, err in zip(T_robot_tcp_list, err_ls_sn):    \n",
    "    mesh_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.05+err)\n",
    "    mesh_frame.transform(T_robot_tcp)\n",
    "    mesh_frames.append(mesh_frame)\n",
    "o3d.visualization.draw_geometries(mesh_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Merged Pointclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "first_rgb = Image.open(vl.get_rgb_file(1))\n",
    "K_o3d = o3d.camera.PinholeCameraIntrinsic()\n",
    "K_o3d.set_intrinsics(first_rgb.size[1], first_rgb.size[0],\n",
    "                     K[0, 0], K[1, 1], K[0, 2], K[1, 2])\n",
    "\n",
    "pcd_list = []\n",
    "for i in range(len(vl)):\n",
    "    try:\n",
    "        rgb_file = Image.open(vl.get_rgb_file(i))\n",
    "        depth_file = Image.open(vl.get_depth_file(i))\n",
    "        T_c = vl.get_cam_pose(i)\n",
    "        T_r = vl.get_robot_pose(i)\n",
    "        depth_scaling = vl.get_robot_pose(i, return_dict=True)[1][\"depth_scaling\"]\n",
    "    except (FileNotFoundError, ValueError):\n",
    "        continue\n",
    "    \n",
    "    rgb = o3d.geometry.Image(np.array(rgb_file))\n",
    "    depth = o3d.geometry.Image(np.array(depth_file).astype(np.uint16))\n",
    "    rgbd = o3d.geometry.RGBDImage.create_from_color_and_depth(rgb, depth,\n",
    "                                          depth_scale=1.0/depth_scaling, depth_trunc=1.0,\n",
    "                                          convert_rgb_to_intensity=False)\n",
    "    pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd, K_o3d)\n",
    "    \n",
    "    #T_est = T_r @ T_calib\n",
    "    T_est = np.linalg.inv(T_c)\n",
    "    pcd.transform(T_est)\n",
    "    pcd_list.append(pcd)\n",
    "\n",
    "# sum pointclouds for easier visualization\n",
    "pcd_all = pcd_list[0]\n",
    "for pcd_cur in pcd_list[1:]:\n",
    "    pcd_all += pcd_cur\n",
    "o3d.visualization.draw_geometries([pcd_all])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40de0b164a2ead70a1213ee87ce739cfc5594d2111c42683eb8f5e0739ba5537"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
