{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RGB-D Fusion\n",
    "\n",
    "This exercise will describe how to fuse images from known poses into one pointcloud.\n",
    "\n",
    "Note: if the interactive viewer does not work you may have to restart the notebook with `%matplotlib widget` instead of `%matplotlib notebook`\n",
    "\n",
    "Note: Start a local jupyter notebook server in the command line via` jupter notebook --port xxxx`. Use the vs-code pop up to open jupyter in your local server.\n",
    "\n",
    "Note: we use [Open3D](http://www.open3d.org/) for this exercise. Unfortunately the visualization requires OpenGL, which fails for setups like ours, where the visualization and the code run on separate machines. A minimal working solution (with drawbacks in quality) is to start a jupyter notebook on one of the pool machines (not the login) and use vscode for port-forwarding it to your local browser.\n",
    "**The recommended way is to run everything on your local machine**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "1. Install requirements.txt with pip into your conda environment.\n",
    "2. Download and extract data (if not running on the pool). Update the variable DATA_PATH below as appropriate.\n",
    "\n",
    "~~~bash\n",
    "# Download the data required for this exercise (30Mb)\n",
    "wget https://lmb.informatik.uni-freiburg.de/people/argusm/wd_40.tar\n",
    "tar -xvf wd_40.tar\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"wd_40\"  # if downloaded to the same folder as the notebook\n",
    "# BASE_PATH = \"/project/cv-ws2223/shared-data1/dataset_wd_40\" # if running on the pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "# %matplotlib widget\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "try:\n",
    "    import open3d as o3d\n",
    "    print(\"open3d loaded\")\n",
    "except ImportError:\n",
    "    print(\"open3d not loaded\")\n",
    "    pass\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "np.set_printoptions(suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Recorded Images\n",
    "\n",
    "This cell defines a data class that loads images and data from files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "class ViewLoader:\n",
    "    def __init__(self, base_path):\n",
    "        self.base_path = base_path\n",
    "        assert os.path.isdir(base_path)\n",
    "        files = sorted(os.listdir(self.base_path))\n",
    "        files = [f for f in files if (f.startswith(\"rgb_\") and f.endswith(\".png\"))]\n",
    "        self.start_idx = 2  # image 0 doesnt exist, image 1 is faulty, start at 2\n",
    "        self.max_idx = int(files[-1].replace(\"rgb_\", \"\").replace(\".png\", \"\"))\n",
    "        print(f\"Loaded {self.max_idx+1} images.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.max_idx + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.get_rgbdp(idx)\n",
    "\n",
    "    def get_info(self):\n",
    "        info = {\n",
    "            \"camera\": {\"calibration\":\n",
    "            {\"width\": 640, \"height\": 480,\n",
    "            \"fx\": 617.8902587890625, \"fy\": 617.8903198242188,\n",
    "            \"ppx\": 315.20367431640625, \"ppy\": 245.70614624023438}}\n",
    "        }\n",
    "        return info\n",
    "\n",
    "    def get_intrinsics(self):\n",
    "        info = self.get_info()\n",
    "        calib = info[\"camera\"][\"calibration\"]\n",
    "        return calib\n",
    "\n",
    "    def get_K(self):\n",
    "        calib = self.get_intrinsics()\n",
    "        cam_intrinsic = np.eye(3)\n",
    "        cam_intrinsic[0, 0] = calib[\"fx\"]\n",
    "        cam_intrinsic[1, 1] = calib[\"fy\"]\n",
    "        cam_intrinsic[0, 2] = calib[\"ppx\"]\n",
    "        cam_intrinsic[1, 2] = calib[\"ppy\"]\n",
    "        return cam_intrinsic\n",
    "\n",
    "    def get_robot_pose(self, idx, return_dict=False):\n",
    "        # 4x4 matrix position und drehung vom TCP (6 freiheitsgrade)\n",
    "        pose_file = os.path.join(self.base_path, \"pose_{0:04d}.json\".format(idx) )\n",
    "        with open(pose_file,\"rb\") as f_obj:\n",
    "            pose = json.load(f_obj)\n",
    "        pose_m = np.eye(4)\n",
    "        pose_m[:3, :3] = R.from_euler(\"xyz\", [pose[x] for x in ['rot_x', 'rot_y', 'rot_z']]).as_matrix()\n",
    "        pose_m[:3, 3] = [pose[x] for x in ['x', 'y', 'z']]\n",
    "        if return_dict:\n",
    "            return pose_m, pose\n",
    "        else:\n",
    "            return pose_m\n",
    "\n",
    "    def get_rgb_file(self, idx):\n",
    "        rgb_file = os.path.join(self.base_path, \"rgb_{0:04d}.png\".format(idx) )\n",
    "        return rgb_file\n",
    "\n",
    "    def get_depth_file(self, idx):\n",
    "        depth_file = os.path.join(self.base_path, \"depth_{0:04d}.png\".format(idx) )\n",
    "        return depth_file\n",
    "\n",
    "    def get_rgbdp(self, idx):\n",
    "        rgb_file = self.get_rgb_file(idx)\n",
    "        depth_file = self.get_depth_file(idx)\n",
    "        pose_m, pose_d = self.get_robot_pose(idx, True)\n",
    "        # depth\n",
    "        depth_scaling = pose_d[\"depth_scaling\"]\n",
    "        rgb  = np.asarray(Image.open(rgb_file))\n",
    "        depth = np.asarray(Image.open(depth_file), dtype=np.float32) * depth_scaling\n",
    "        return rgb, depth, pose_m\n",
    "\n",
    "    def get_cam_pose(self, idx, marker_dir=\"pose_marker_one\"):\n",
    "        marker_dir = os.path.join(self.base_path, marker_dir)\n",
    "        fn  = \"{0:08d}.json\".format(idx)\n",
    "        pose_fn = os.path.join(marker_dir, fn)\n",
    "        with open(pose_fn, \"r\") as fo:\n",
    "            T = np.array(json.load(fo))\n",
    "        return T\n",
    "\n",
    "    def get_projection_matrix(self):\n",
    "        # returns a 4x3 projection matrix using the intrinsics\n",
    "        intr = self.get_intrinsics()\n",
    "        cam_mat = np.array([[intr['fx'], 0, intr['ppx'], 0],\n",
    "                            [0, intr['fy'], intr['ppy'], 0],\n",
    "                            [0, 0, 1, 0]])\n",
    "        assert cam_mat.shape == (3, 4)\n",
    "        return cam_mat\n",
    "\n",
    "\n",
    "    def project(self, X):\n",
    "        \"\"\"\n",
    "        Project an (homogenous) cartesian coordinate into the camera frame.\n",
    "        \"\"\"\n",
    "        assert X.shape == (4,), \"Expects input as np.array([x, y, z, 1])\"\n",
    "\n",
    "        # project input to homogenous 2D coordinates given the camera projection matrix\n",
    "        x = self.get_projection_matrix() @ X\n",
    "\n",
    "        # convert homogenous 2D to euclidean 2D and round to integer pixels\n",
    "        result = np.round(x[0:2] / x[2]).astype(int)\n",
    "\n",
    "        # check if the output is a valid point in the image\n",
    "        width, height = self.get_intrinsics()['width'], self.get_intrinsics()['height']\n",
    "        if not (0 <= result[0] < width and 0 <= result[1] < height):\n",
    "            log.warning(\"Projected point outside of image bounds\")\n",
    "\n",
    "        return result[0], result[1]\n",
    "\n",
    "vl = ViewLoader(base_path=BASE_PATH)\n",
    "print(\"camera calibration:\")\n",
    "camera_calibration = vl.get_K()\n",
    "K = np.array(camera_calibration)\n",
    "print(K.round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display loaded RGB images and corresponding depth maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets, Layout, interact\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.set_title(\"RGB\")\n",
    "ax.set_axis_off()\n",
    "\n",
    "line = None\n",
    "def update(w):\n",
    "    global line\n",
    "    image, depth, pose = vl.get_rgbdp(w)\n",
    "    if line is None:\n",
    "        line = ax.imshow(np.asarray(image))\n",
    "    else:\n",
    "        line.set_data(np.asarray(image))\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "slider_w = widgets.IntSlider(min=vl.start_idx, max=vl.max_idx-1, step=1, value=0,\n",
    "                             layout=Layout(width='70%'))\n",
    "interact(update, w=slider_w)\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "ax.set_title(\"Depth\")\n",
    "ax.set_axis_off()\n",
    "\n",
    "line = None\n",
    "def update(w):\n",
    "    global line\n",
    "    image, depth, pose = vl.get_rgbdp(w)\n",
    "    depth_for_plot = np.asarray(depth)\n",
    "    if line is None:\n",
    "        line = ax.imshow(depth_for_plot)\n",
    "    else:\n",
    "        line.set_data(depth_for_plot)\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "slider_w = widgets.IntSlider(min=vl.start_idx, max=vl.max_idx-1, step=1, value=0,\n",
    "                             layout=Layout(width='70%'))\n",
    "interact(update, w=slider_w)\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Show Marker Detection Results.\n",
    "\n",
    "To simplify things marker detection has already been run. Next we want to verify its results.\n",
    "Draw a coordinate frame into each image for which we have detection results.\n",
    "The coordinate frame should have axis lengths of 10cm, with x=red, y=green, and z=blue.\n",
    "This can be done using `PIL.ImageDraw`:\n",
    "\n",
    "~~~python\n",
    "im = Image.fromarray(numpy_image)\n",
    "draw = ImageDraw.Draw(im)\n",
    "draw.line((x1, y1, x2, y2), fill=\"red\", width=5)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "\n",
    "def show_marker_pose(image, T_cam_object):\n",
    "    \"\"\"\n",
    "    draw the coordinate frame into each image for which we have detection results\n",
    "\n",
    "    Arguments:\n",
    "        image: image as numpy.ndarray\n",
    "        T_cam_object: transformation matrix from object to camera space, shape (4, 4)\n",
    "            x_cam = T_cam_object @ x\n",
    "\n",
    "    Returns:\n",
    "        im: image as numpy.ndarray with coordinate frame drawn on it\n",
    "    \"\"\"\n",
    "    # first we define our points in object space\n",
    "    # we want to draw 3 lines going from the center into x, y and z direction\n",
    "    dist = 0.1\n",
    "    c = np.array([0, 0, 0, 1])\n",
    "    x = np.array([dist, 0, 0, 1])\n",
    "    y = np.array([0, dist, 0, 1])\n",
    "    z = np.array([0, 0, dist, 1])\n",
    "\n",
    "    # START TODO\n",
    "    # 1. transform the above points from object to camera space\n",
    "    # 2. project the points from camera to image space using vl.project\n",
    "    # 3. draw the 3 lines using pillow as described above\n",
    "    raise NotImplementedError\n",
    "    # END TODO\n",
    "\n",
    "    # convert image back to numpy\n",
    "    return np.asarray(im)\n",
    "\n",
    "line = None\n",
    "\n",
    "def update(w):\n",
    "    global line\n",
    "    image, depth, pose = vl.get_rgbdp(w)\n",
    "    try:\n",
    "        T_cam_object = vl.get_cam_pose(w)\n",
    "    except FileNotFoundError:\n",
    "        print(\"No pose estimation.\")\n",
    "        line.set_data(image)\n",
    "        return\n",
    "    image_m = show_marker_pose(image, T_cam_object)\n",
    "\n",
    "    if line is None:\n",
    "        line = ax.imshow(image_m)\n",
    "    else:\n",
    "        line.set_data(image_m)\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.set_axis_off()\n",
    "update(2)\n",
    "\n",
    "slider_w = widgets.IntSlider(min=2, max=len(vl)-1, step=1, value=0,\n",
    "                             layout=Layout(width='70%'))\n",
    "interact(update, w=slider_w)\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Display Merged Pointclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import open3d as o3d\n",
    "\n",
    "# create camera intrinsics\n",
    "first_rgb = Image.open(vl.get_rgb_file(vl.start_idx))\n",
    "K_o3d = o3d.camera.PinholeCameraIntrinsic()\n",
    "K_o3d.set_intrinsics(first_rgb.size[1], first_rgb.size[0],\n",
    "                     K[0, 0], K[1, 1], K[0, 2], K[1, 2])\n",
    "\n",
    "pcd_list = []\n",
    "for i in range(vl.start_idx, vl.max_idx):\n",
    "    try:\n",
    "        rgb_file = Image.open(vl.get_rgb_file(i))\n",
    "        depth_file = Image.open(vl.get_depth_file(i))\n",
    "        T_c = vl.get_cam_pose(i)\n",
    "        T_r = vl.get_robot_pose(i)\n",
    "        depth_scaling = vl.get_robot_pose(i, return_dict=True)[1][\"depth_scaling\"]\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "    # create the pointcloud given RGB and depth\n",
    "    rgb = o3d.geometry.Image(np.array(rgb_file))\n",
    "    depth = o3d.geometry.Image(np.array(depth_file).astype(np.uint16))\n",
    "    rgbd = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "        rgb, depth, depth_scale=1.0/depth_scaling, depth_trunc=1.0,\n",
    "        convert_rgb_to_intensity=False)\n",
    "    pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd, K_o3d)\n",
    "\n",
    "    # transform the pointcloud from camera space to object space, so that\n",
    "    # all pointclouds use the same coordinate system\n",
    "    T_est = np.linalg.inv(T_c)\n",
    "    pcd.transform(T_est)\n",
    "\n",
    "    pcd_list.append(pcd)\n",
    "\n",
    "# merge pointclouds for easier visualization\n",
    "pcd_all = pcd_list[0]\n",
    "for pcd_cur in pcd_list[1:]:\n",
    "    pcd_all += pcd_cur\n",
    "o3d.visualization.draw_geometries([pcd_all], window_name='Merged pointcloud')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. improve the pointcloud\n",
    "- remove the gripper\n",
    "- unify the points on the surface\n",
    "- extract a mesh\n",
    "- crop the mesh to show only the object\n",
    "\n",
    "Just try out many things. You can find some inspiration in [tutorials](http://www.open3d.org/docs/0.8.0/index.html#tutorial-index). Make sure the docs version and your installed version match, since some functionality has changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 remove the gripper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the depth values to create a mask for the gripper\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 6))\n",
    "image, depth, pose = vl.get_rgbdp(10)\n",
    "ax = axes[0]\n",
    "ax.imshow(depth)\n",
    "ax.set_title('original depth')\n",
    "# plt.show()\n",
    "\n",
    "# find a depth threshold that works across images\n",
    "\n",
    "# example: 0.25 (works ok)\n",
    "depth_new = depth.copy()\n",
    "depth_new[depth < 0.25] = 0\n",
    "ax = axes[1]\n",
    "ax.set_title(\"depth no gripper\")\n",
    "ax.imshow(depth_new)\n",
    "\n",
    "# example: median of the image (removes too much)\n",
    "depth_median = depth.copy()\n",
    "median = np.median(depth_median)\n",
    "print(f\"median: {median}\")\n",
    "depth_median[depth_median < median] = 0\n",
    "ax = axes[2]\n",
    "ax.set_title(\"threshold median\")\n",
    "ax.imshow(depth_median)\n",
    "\n",
    "# remove tick labels\n",
    "for ax in axes:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new pointcloud, removing all points with depth < depth_mask by setting their depth to 0\n",
    "# if done correctly, the gripper should be removed\n",
    "depth_mask = 0.25\n",
    "\n",
    "def mask_depth(raw_depth, scaled_depth):\n",
    "    raw_depth = np.array(raw_depth)\n",
    "    # START TODO\n",
    "    # 1. create a mask given the scaled depth and the threshold\n",
    "    # 2. set the raw_depth to 0 for all masked points.\n",
    "    # this way, they will be behind the camera and hidden\n",
    "    raise NotImplementedError\n",
    "    # END TODO\n",
    "    return raw_depth\n",
    "\n",
    "first_rgb = Image.open(vl.get_rgb_file(2))\n",
    "K_o3d = o3d.camera.PinholeCameraIntrinsic()\n",
    "K_o3d.set_intrinsics(first_rgb.size[0], first_rgb.size[1],\n",
    "                     K[0, 0], K[1, 1], K[0, 2], K[1, 2])\n",
    "\n",
    "pcd_list = []\n",
    "for i in range(2, len(vl)):\n",
    "    try:\n",
    "        rgb_file = Image.open(vl.get_rgb_file(i))\n",
    "        depth_file = Image.open(vl.get_depth_file(i))\n",
    "        T_c = vl.get_cam_pose(i)\n",
    "        T_r = vl.get_robot_pose(i)\n",
    "        depth_scaling = vl.get_robot_pose(i, return_dict=True)[1][\"depth_scaling\"]\n",
    "\n",
    "        # get scaled depth and use it to mask the raw depth data in depth_file\n",
    "        _, scaled_depth_, _ = vl.get_rgbdp(i)\n",
    "        depth_file = mask_depth(depth_file, scaled_depth_)\n",
    "    except (FileNotFoundError, ValueError):\n",
    "        continue\n",
    "\n",
    "    # create and transform pointcloud as before\n",
    "    rgb = o3d.geometry.Image(np.array(rgb_file))\n",
    "    depth = o3d.geometry.Image(depth_file.astype(np.uint16))\n",
    "    rgbd = o3d.geometry.RGBDImage.create_from_color_and_depth(rgb, depth,\n",
    "                                          depth_scale=1.0/depth_scaling, depth_trunc=1.0,\n",
    "                                          convert_rgb_to_intensity=False)\n",
    "    pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbd, K_o3d)\n",
    "    T_est = np.linalg.inv(T_c)\n",
    "    pcd.transform(T_est)\n",
    "    pcd_list.append(pcd)\n",
    "\n",
    "pcd_all = pcd_list[0]\n",
    "for pcd_cur in pcd_list[1:]:\n",
    "    pcd_all += pcd_cur\n",
    "o3d.visualization.draw_geometries([pcd_all], window_name='Gripper removed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO\n",
    "# Select only every 5th point by using uniform_down_sample\n",
    "raise NotImplementedError\n",
    "# END TODO\n",
    "o3d.visualization.draw_geometries([uni_down_pcd], window_name='Uniform point downsample')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers\n",
    "# Note: tested for open3d.__version__ == 0.8.0\n",
    "#       open3d refactored this syntax between different versions\n",
    "#       make sure to select the correct version in the docs\n",
    "#       http://www.open3d.org/docs/0.8.0/\n",
    "def display_inlier_outlier(cloud, ind):\n",
    "    # v0.8\n",
    "    # inlier_cloud = cloud.select_down_sample(ind)\n",
    "    # outlier_cloud = cloud.select_down_sample(ind, invert=True)\n",
    "    # v0.12\n",
    "    inlier_cloud = cloud.select_by_index(ind)\n",
    "    outlier_cloud = cloud.select_by_index(ind, invert=True)\n",
    "\n",
    "    print(\"Showing outliers (red) and inliers (gray): \")\n",
    "    outlier_cloud.paint_uniform_color([1, 0, 0])\n",
    "    inlier_cloud.paint_uniform_color([0.8, 0.8, 0.8])\n",
    "    o3d.visualization.draw_geometries([inlier_cloud, outlier_cloud],\n",
    "                                      window_name=\"Red outliers, gray inliers\")\n",
    "\n",
    "# START TODO\n",
    "# Downsample the point cloud with a voxel of 0.002 using voxel_down_sample\n",
    "raise NotImplementedError\n",
    "# END TODO\n",
    "o3d.visualization.draw_geometries([voxel_down_pcd], window_name='Voxel downsample')\n",
    "\n",
    "# START TODO\n",
    "# Remove statistical outliers with remove_statistical_outlier on the voxel downsampled cloud\n",
    "raise NotImplementedError\n",
    "# END TODO\n",
    "o3d.visualization.draw_geometries([no_outlier_pcd], window_name='Remove_statistical_outlier')\n",
    "\n",
    "# show which points where detected as outliers\n",
    "display_inlier_outlier(voxel_down_pcd, ind)\n",
    "\n",
    "# # This runs for a very long time\n",
    "# # print(\"Radius oulier removal\")\n",
    "# cl,ind = voxel_down_pcd.remove_radius_outlier(nb_points=16, radius=0.1)\n",
    "# o3d.visualization.draw_geometries([cl], window_name='Remove_statistical_outlier')\n",
    "# display_inlier_outlier(voxel_down_pcd, ind)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 extract a mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pcd_all.get_min_bound())\n",
    "print(pcd_all.get_max_bound())\n",
    "\n",
    "# create the volume for the mesh\n",
    "\n",
    "# - o3d v0.8\n",
    "# volume = o3d.integration.ScalableTSDFVolume(\n",
    "#     voxel_length=1.0 / 512.0,\n",
    "#     sdf_trunc=0.10,\n",
    "#     color_type=o3d.integration.TSDFVolumeColorType.RGB8)\n",
    "# - o3d v0.12\n",
    "volume = o3d.pipelines.integration.ScalableTSDFVolume(\n",
    "    voxel_length=1.0 / 512.0,\n",
    "    sdf_trunc=0.10,\n",
    "    color_type=o3d.pipelines.integration.TSDFVolumeColorType.RGB8)\n",
    "\n",
    "first_rgb = Image.open(vl.get_rgb_file(2))\n",
    "K_o3d = o3d.camera.PinholeCameraIntrinsic()\n",
    "K_o3d.set_intrinsics(first_rgb.size[0], first_rgb.size[1],\n",
    "                     K[0, 0], K[1, 1], K[0, 2], K[1, 2])\n",
    "for i in range(2, len(vl)):\n",
    "    if i % 10 == 0:\n",
    "        print(\"Integrate {:d}-th image into the volume.\".format(i))\n",
    "    try:\n",
    "        rgb_file = Image.open(vl.get_rgb_file(i))\n",
    "        depth_file = Image.open(vl.get_depth_file(i))\n",
    "        T_c = vl.get_cam_pose(i)\n",
    "        T_r = vl.get_robot_pose(i)\n",
    "        depth_scaling = vl.get_robot_pose(i, return_dict=True)[1][\"depth_scaling\"]\n",
    "        depth_file = np.array(depth_file)\n",
    "        # get scaled depth and use it to mask the raw depth data in depth_file\n",
    "        _, scaled_depth_, _ = vl.get_rgbdp(i)\n",
    "        depth_file = mask_depth(depth_file, scaled_depth_)\n",
    "    except (FileNotFoundError, ValueError):\n",
    "        continue\n",
    "\n",
    "    # create the pointcloud as before\n",
    "    rgb = o3d.geometry.Image(np.array(rgb_file))\n",
    "    depth = o3d.geometry.Image(depth_file.astype(np.uint16))\n",
    "    rgbd = o3d.geometry.RGBDImage.create_from_color_and_depth(rgb, depth,\n",
    "                                          depth_scale=1.0/depth_scaling, depth_trunc=1.0,\n",
    "                                          convert_rgb_to_intensity=False)\n",
    "\n",
    "    # START TODO\n",
    "    # use volume.integrate to integrate the pointcloud into the mesh\n",
    "    raise NotImplementedError\n",
    "    # END TODO\n",
    "\n",
    "print(\"Extract a triangle mesh from the volume and visualize it.\")\n",
    "print(volume.extract_voxel_point_cloud())\n",
    "\n",
    "mesh = volume.extract_triangle_mesh()\n",
    "pcd_volume = volume.extract_point_cloud()\n",
    "mesh.compute_vertex_normals()\n",
    "o3d.visualization.draw_geometries([pcd_volume], window_name='Pointcloud from volume')\n",
    "o3d.visualization.draw_geometries([mesh], window_name='Mesh from volume')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 crop the mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop the mesh\n",
    "print(mesh.get_min_bound())\n",
    "print(mesh.get_max_bound())\n",
    "# START TODO\n",
    "# to create a bounding box, define center (3d vector), angle (scalar, radians) and size (3d vector)\n",
    "raise NotImplementedError\n",
    "# END TODO\n",
    "\n",
    "# create the bounding box given the parameters\n",
    "obb = o3d.geometry.OrientedBoundingBox(\n",
    "    np.array(box_center),\n",
    "    np.array([[np.cos(angle),-np.sin(angle),0],[np.sin(angle),np.cos(angle),0],[0,0,1]]), # rotations\n",
    "    np.array(box_size)\n",
    ")\n",
    "\n",
    "o3d.visualization.draw_geometries([mesh, obb], window_name='Mesh and bounding box')\n",
    "\n",
    "cropped_mesh = mesh.crop(obb)\n",
    "o3d.visualization.draw_geometries([cropped_mesh], window_name='Cropped mesh')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40de0b164a2ead70a1213ee87ce739cfc5594d2111c42683eb8f5e0739ba5537"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}